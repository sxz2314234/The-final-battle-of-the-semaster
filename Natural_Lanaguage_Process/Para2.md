# 第二章: 数学基础

## 熵

可以被视为描述一个随机变量的不确定性的数量, 一个随机变量的熵越大, 它的不确定性越大.

$$
H(X)=-\sum_{x\in X}p(x)log_2p(x)
$$

**最大熵模型**: 关于未知分布最合理的推断应该是符合符合已知知识最不确定或最大随机的判断.

## 联合熵

假设有两个随机变量 X 和 Y，它们之间可能存在某种关联或依赖关系。联合熵 H(X, Y) 表示在考虑了 X 和 Y 的同时，系统的总不确定性。计算联合熵的方法与计算单个变量的熵类似，通常使用概率分布来描述不同取值的概率，并应用熵的定义来计算。公式如下:

$$
H(X,Y)=-\sum_{x\in X}\sum_{y\in Y}P(x,y)log_2P(x,y)
$$

## 条件熵

给定随机变量X的情况下, 随机变量Y的条件熵

## 相对熵

又称KL距离,用以衡量事件实际的随机分布与真实随机分布的差距.公式如下:

$$
H(X)=\sum_{i=1}^n p(x_i)log_2p(x_i)-\sum_{i=1}^n p(x_i)log_2q(x_i)
$$

**交叉熵**: 为相对熵的后半部分



在设计语言模型时, 我们通常用困惑度来代替交叉熵衡量语言模型的好坏. 语言模型设计的任务是寻找困惑度最小的模型,使其最接近真实的语言.

互信息: $ I(X;Y)=H(X)-H(X|Y) $

互信息$ I(X;Y) $是在知道了Y的值以后X的不确定性的减少量

在汉语分词研究中, 可以用**双字耦合度**的概念代替互信息,估计两个汉字的结合程度.
