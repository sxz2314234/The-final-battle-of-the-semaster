# 第十章: Word2vec词向量

## one-hot表示

大部分基于规则和基于统计的自然语言处理任务把词看作原子符号

采用**向量空间模型**表示, 每个词都是茫茫0海中的一个. $[0\ 0\ 0 \ 0\ 0\ 1\ 0]$

**问题 :**

* 向量维度较高
  
  * 20K( 口语 ) - 50K( PTB ) - 500K( 大词表 ) - 13M( 谷歌1T )

* 语义鸿沟
  
  * 采用one-hot表示, 任意两个词之间都是孤立的 : $[0\ 0\ 0 \ 1\ 0\ 0\ 0]\ AND\ [0\ 0\ 0 \ 0\ 0\ 1\ 0]=0$

## Word2vec

将单词转变为向量表示

## 分布式假设

具有相同上下文语境的词, 有相似的含义.

## CBOW vs. Skip-gram模型

适合于数据集较小的情况, 而Skip-gram在大型语料中表现的更好.

1. 连续词袋模型( CBOW ): 用周围词预测中间词的方法

2. skip-gram模型: 通过中间词预测周围词的概率

## Hierarchical softmax

采用**霍夫曼树**来替换了原先的从隐藏层到输出层的矩阵$W$.霍夫曼树是带劝路径长度最短的树, 保证了词频高的单词的路径短, 词频相对低的单词的路径常.

## 负采样

负采样是另一种用来提高Wordvec效率的方法, 为每个训练实例都是提供负例.

## 词向量模型应用

语义相关性, 同义词检测, 单词类比, 文本分类, 命名实体识别, 词性标注

## 文本表示方法

1. Bag-of-words词袋模型: one-hot, tf-idf, textrank等

2. 主题模型: LSA( SVD ), pLSA, LDA;

3. 基于词向量的固定表征: word2vec, fastText, glove

4. 基于词向量的动态表征: elmo, GPT, BERT, ERNIE
