# 第五章:语言模型

## 基本概念

为了保证条件概率在$i=1$时有意义,同时为了保证句子内所有字符串的概率和为1,可以在句子首尾两端增加两个标志即

**\<BOS>**$w_1w_2...w_m$**\<EOS>**

### n元文法模型

说明:

1. $w_i$可以是字, 词, 短语或词类等等, 统称为统计基元. 通常以词代之

2. $w_i$的概率是由$w_1,w_2,...,w_{i-1}$决定,由特定的一组$w_1,...,w_i $构成的一个序列, 称为$w_i $的历史

举例:

给定句子: John read a book

增加标记: \<BOS> John read a book \<EOS>

* Unigram : \<BOS>, John, read, a, book, \<EOS>

* Bigram : (\<BOS>John),(John read),(read a),(a book),(book \<EOS>)

* Trigram : (\<BOS>John read),(John read a),(read a book),(a book \<EOS>)

则基于2元文法的概率为:

$$
p(John\ read\ a\ book)=p(John|<BOS>)\times p(read|John)\times p(a|read)\times p(book|a)\times p(<EOS>|book)
$$

## 参数估计

**两个重要的概念 :**

* 训练语料
  
  * 用于建立模型, 确定参数的已知语料 ( 计算词的频度 )

* 最大似然估计
  
  * 用相对频率计算概率的方法



对于n-gram, 参数$p(w_i|w^{i-1}_{i-n+1}) $可由最大似然估计得

$$
p(w_m|w_1w_2...w_{m-1})=\frac{序列w_1w_2...w_m出现的次数}{序列w_1,w_2,...,w_{m-1}出现的次数}
$$

## 数据平滑





**加一法** 基本思想: 每种情况出现的次数加1

例如,对于uni-gram(一元文法)设$w_1,w_2,w_3$三个词, 概率分别为: 1/3, 0, 2/3.

加1后的情况 ? 答: 2/6, 1/6, 2/3

对于2-gram有

$$
p(w_i|w_{i-1})=\frac{1+c(w_{i-1}w_i)}{\sum_{w_i}1+c(w_{i-1}w_i)}\\
=\frac{1+c(w_{i-1}w_i)}{|V|+\sum_{w_i}c(w_{i-1}w_i)}
$$

其中, V为被考虑语料的词汇量

减值法: 修改训练样本中事件的实际计数，使样本中(实际出现的)不同事件的概率之和小于1，剩余的概率量分配给未见概率。
a) Good-Turing 估计: 对非0事件按公式削减出现的次数，节留出来的概率均分给0概率事件。

b) Back-off （Katz 后退）法: 当某一事件在样本中出现的频率大于阈值K (通常取K 为0 或1)时，运用最大似然估计的减值法来估计其概率，否则，使用低阶的，即(n-1)gram 的概率替代n-gram 概率。
c) 绝对减值法(Absolute discounting ): 从每个计数r 中减去同样的量，剩余的概率量由未见事件均分。
d) 线性减值法: 从每个计数r 中减去与该计数成正比的量(减值函数为线性的)，剩余概率量被n0个未见事件均分。



## 语言模型的自适应性

1. **基于缓存的语言模型**（在文本中刚刚出现过的一些词在后边的句子中再次出现的可能性往往较大）

2. **基于混合方法的语言模型**由于大规模训练语料本身是异源的，来自不同领域的语料无论在主题方面, 还是在风格方面, ,或者两者都有一定的差异，而测试语料一般是同源的，因此，为了获得最佳性能，语言模型必须适应各种不同类型的语料对其性能的影响）

3. **基于最大熵的语言模型**（通过结合不同信息源的信息构建一个语言模型。每个信息源提供一组关于模型参数的约束条件，在所有满足约束的模型中，选择熵最大的模型）


