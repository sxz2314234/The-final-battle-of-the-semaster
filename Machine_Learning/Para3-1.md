# 支持向量机

使用的是**最大间隔**思想: 寻找参数$ \overrightarrow{\omega} $和b使得间隔最大.

![](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/Machine_Learning/2024-06-04_15-37.png)

对于支持向量机来说，数据点若是p维向量，我们用p−1维的超平面来分开这些点。但是可能有许多超平面可以把数据分类。最佳超平面的一个合理选择就是以最大间隔把两个类分开的超平面。因此，SVM选择能够使离超平面最近的数据点的到超平面距离最大的超平面。



为了解决更加复杂问题, 支持向量机学习方法有一些由简至繁 :

* 线性可分SVM
  
  > 当训练数据线性可分时，通过硬间隔(hard margin，什么是硬、软间隔下面会讲)最大化可以学习得到一个线性分类器，即硬间隔SVM.即两个数据集被完全的分开.

* 线性SVM
  
  > 当训练数据不能线性可分但是可以近似线性可分时，通过软间隔(soft margin)最大化也可以学习到一个线性分类器，即软间隔SVM。存在一小部分数据不能被分开.

* 非线性SVM
  
  > 当训练数据线性不可分时，通过使用核技巧(kernel trick)和软间隔最大化，可以学习到一个非线性SVM。
