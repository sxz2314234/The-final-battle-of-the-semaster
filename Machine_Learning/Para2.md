# 第二章: 机器学习概述

## 机器学习概念:

通过算法使得机器能从大量数据中学习规律,从而对新的样本做决策.

### 机器学习的三要素:

1. 模型
   
   * 线性方法: $ f(\overrightarrow{x},\theta)=\overrightarrow{w}^T\overrightarrow{x} +b$
   
   * 广义线性方法:$ f(\overrightarrow{w}^T{\psi(\overrightarrow x)})+b $

2. 学习准则
   
   * **期望风险**是全局的,基于所有样本点损失函数最小化.期望风险是全局最优,是理想化,的不可求的.
   
   * **经验风险**是局部的,基于训练集所有样本点损失函数最小化.经验风险是局部最优,是现实可求得.

3. 优化准则

## 常见的机器学习问题与算法:

1. 回归:
   
   * 线性回归
   
   * 广义线性回归
   
   * KNN
   
   * 神经网络

2. 分类
   
   * KNN
   
   * SVM(Support Vector Machine)
   
   * 决策树
   
   * 贝叶斯分类器
   
   * 神经网络

3. 聚类
   
   * K-Means
   
   * GMM
   
   * DBSCAN
   
   * 层次聚类
   
   * 谱聚类

## 梯度下降算法大家族( BGD,SGD,MBGD )

### 批量梯度下降算法( Batch Gradient Descent )

由于我们有m个样本,这里求梯度的时候就用了**所有m个**样本的梯度数据.

公式: $ \theta_i=\theta_i-\alpha \sum_{j=1}^m(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})-y_i)x_i^{(j)} $

### 随机梯度下降算法( Stochastic Gradient Descent )

其与BGD算法相似,区别在于求梯度时没有用所有的m个样本的数据,而是仅仅**选取一个样本j**来求梯度.对应的更新公式是:

$$
\theta_i=\theta_i-\alpha (h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})-y_i)x_i^{(j)}
$$

$$

随机梯度下降法，和4.1的批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。

那么，有没有一个中庸的办法能够结合两种方法的优点呢？有！这就是4.3的小批量梯度下降法。

### 小批量梯度下降算法( Mini-batch Gradient Descent )

小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，1<x<m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。对应的更新公式是：

$$
\theta_i=\theta_i-\alpha \sum_{j=t}^{t+x-1}(h_\theta(x_0^{(j)},x_1^{(j)},...,x_n^{(j)})-y_i)x_i^{(j)}
$$

$$

## 过拟合

在训练集上错误很低,但在未知数据上错误率很高.

## 线性回归( Liner Regression )

**模型**: $f(\overrightarrow x;\overrightarrow w,b)=\overrightarrow w^T\overrightarrow x+b  $

**线性回归优化方法** :

* 经验风险最小化( 最小二乘法 )

* 结构风险最小化( 岭回归 )

* 最大似然估计

* 最大后验估计

### 经验风险最小化

**学习准则:**

$$
R(\overrightarrow w)=\frac{1}{2}\sum_{n=1}^N(y^{(n)}-\overrightarrow w^Tx^{(n)})^2=\frac{1}{2}{\Vert y-X^T\overrightarrow w\Vert}^2
$$

$$

**优化:** 

$$
\frac{\partial R(w)}{\partial w}=-X(y-X^T\overrightarrow w)
$$

$$

### 结构风险最小化准则

$$
R(\overrightarrow w)=\frac{1}{2}{\Vert y-X^T\overrightarrow w\Vert}^2+\frac{1}{2}\lambda\Vert\overrightarrow w\Vert^2, 
$$

$$

**得到**

$$
\overrightarrow w^*=(XX^T+\lambda I)^{-1}Xy
$$

$$

注: 实际上, 结构风险最小化是在经验风险最小化的基础上进行了正则化,即加了惩罚项,防止模型过拟合.



上述算法是从**函数的梯度**角度出发,接下来从**概率**视角看

### 最大似然估计与最大后验估计

**明确区别**

* 区别一 : 考虑先验信息
  
  * 最大后验估计( MAP )
    
    * MAP即考虑观测数据,也考虑先验信息.
    
    * 目标是找到观测数据和先验信息下最可能的参数
    
    * 理论依据是贝叶斯公式: $ P(\theta|X)=\frac{P(X|\theta)P(\theta)}{P(X)} $
  
  * 最大似然估计( MLE )
    
    * MLE仅考虑观测数据( 在假设已知各数据独立同分布,且满足某一概率分布的前提下 )
    
    * 目标是找到使观测数据出现的可能性最大的参数。

## 常见的机器学习类型

1. 有监督学习

2. 无监督学习

3. 强化学习

## 如何选择一个模型?

**模型选择**

* 拟合能力强的模型一般复杂度高,容易过拟合

* 如果限制模型复杂度,降低拟合能力,可能会欠拟合.

**偏差与方差分解**

## 没有免费午餐定理

基于迭代的最优化算法,不存在某种算法对所有问题( 有限的搜索空间内 )都有效. 如果一个算法对某些问题有效,那么它一定在另一些问题上比纯随机搜索算法差.

## 奥卡姆剃刀原理

如无必要,勿增实体
