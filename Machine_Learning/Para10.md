# 第十章: 模型独立的学习方式

## 集成学习

### Bagging方法

关注降低方差

**Bagging（Bootstrap Aggregating)** 是一个通过不同模型的训练数据集的独立性来提高不同模型之间的独立性。我们在原始训练集上进行有放回的随机采样，得到M 比较小的训练集并训练M 个模型，然后通过投票的方法进行模型集成。

**随机森林 :** 是在Bagging的基础上再引入了**随机特征**，进一步提高每个基模型之间的独立性。在随机森林中，每个基模型都是一棵决策树。采样的随机性, 属性选择的随机性

<img src="file:///home/sxz/The%20Final%20Battle%20of%20the%20Semester/Machine_Learning/2024-06-13_09-32.png" title="" alt="2024-06-13_09-32.png" width="353">

### Boosting方法

按照一定的顺序来先后训练不同的基模型，每个模型都针对前序模型的错误进行专门训练。根据前序模型的结果，来调整训练训练样本的权重，从而增加不同基模型之间的差异性。

**AdaBoost（Adaptive Boosting）算法** : 降低偏差, 可对泛化性能相对弱的学习器构造出很强的集成. 

**主要特点 :**

1. **集成方法**：AdaBoost通过组合多个简单的弱分类器（通常是决策树桩，简单的二叉树）来构建一个更强的分类器。
2. **加权数据集**：在每一轮迭代中，AdaBoost根据前一轮的错误率调整训练数据的权重，使得下一轮训练更加关注之前被错误分类的样本。
3. **自适应性**：其名称中的“Adaptive”来源于它在每一轮迭代中根据弱分类器的表现自适应地调整样本权重和分类器权重。

## 半监督学习

### 自训练( self-Training )

其步骤

- **初始模型训练**：
  
  - 使用初始标记数据集$L$ 训练一个初始分类器 f。

- **标签预测**：
  
  - 用训练好的模型 f 对未标记数据集 U 进行预测，获得预测标签和相应的置信度（例如，预测概率）。

- **选择高置信度样本**：
  
  - 从预测结果中选择那些模型预测标签具有高置信度的样本。设定一个置信度阈值，将超过该阈值的预测结果作为新的伪标签样本。

- **扩展标记数据集**：
  
  - 将选定的高置信度样本及其伪标签加入标记数据集 L，形成扩展后的标记数据集 $L′=L∪\{(x_i​,y_i​)∣x_i​∈U, y_i\}$​是高置信度预测标签}。

- **模型再训练**：
  
  - 使用扩展后的标记数据集 L′ 重新训练分类器 f。

- **迭代**：
  
  - 重复步骤2至5，直至满足停止条件（例如，达到预定的迭代次数或模型性能不再显著提升）。

**优点：**

- **简单易实现**：自训练方法直观且易于实现，只需要在现有的监督学习框架上进行扩展。
- **有效利用未标记数据**：能够在标记数据稀缺的情况下，通过未标记数据的伪标签提升模型性能。
- **适用范围广**：自训练可以与各种类型的模型和数据集结合使用。

**缺点：**

- **误差传播**：如果模型在早期迭代中对未标记数据的预测不准确，错误的伪标签可能会被加入训练集中，导致误差传播。
- **模型依赖性**：自训练方法依赖于初始模型的性能，初始模型的准确性对最终结果有较大影响。
- **高置信度选择**：选择伪标签时需要设定合适的置信度阈值，过高或过低都会影响模型性能。

### 协同训练: 基于不同视角

**主要概念：**

1. **条件独立性**：数据的特征可以分成两组，这两组特征在给定类别标签的条件下是相互独立的。
2. **特征冗余性**：这两组特征能够独立地提供足够的信息来进行分类。

**协同训练的详细步骤：**

1. **特征分离**：
   
   - 将数据集的特征分成两组（例如，特征集1和特征集2），每个分类器使用一组特征进行训练。

2. **初始分类器训练**：
   
   - 使用初始标记数据集 L 的特征集1训练第一个分类器 C1​。
   - 使用初始标记数据集 L 的特征集2训练第二个分类器 C2​。

3. **标签预测**：
   
   - 使用分类器 C1​ 对未标记数据集 U 进行预测，获得预测标签和相应的置信度。
   - 使用分类器 C2​ 对未标记数据集 U 进行预测，获得预测标签和相应的置信度。

4. **高置信度样本选择**：
   
   - 从 C1​ 的预测结果中选择高置信度的样本及其预测标签，将其加入 C2​ 的训练集。
   - 从 C2​ 的预测结果中选择高置信度的样本及其预测标签，将其加入 C1​ 的训练集。

5. **分类器再训练**：
   
   - 使用增强后的训练集重新训练分类器 C1​ 和 C2​。

6. **迭代**：
   
   - 重复步骤3至5，直至满足停止条件（例如，达到预定的迭代次数或模型性能不再显著提升）。

**优点：**

- **增强学习能力**：通过两个分类器的相互增强，可以更好地利用未标记数据，提高模型的泛化能力。
- **适应多样化数据**：适用于具有多种特征表示的数据，尤其是特征可以自然分离的情况。
- **鲁棒性**：通过条件独立的假设，可以减轻某一组特征中的噪声对模型的影响。

**缺点：**

- **特征分离要求**：协同训练要求数据特征能够自然分离成两个条件独立且冗余的子集，这在实际应用中可能不总是成立。
- **高置信度样本选择**：选择高置信度样本的阈值设定可能影响最终结果，过高或过低都会影响模型性能。

## 迁移学习

迁移学习（Transfer Learning）是一种机器学习技术，它将从一个领域（源领域）中学到的知识应用到另一个相关但不同的领域（目标领域）中。迁移学习的核心思想是利用已有的知识来加速和改善新任务的学习过程，特别是在目标领域的数据有限或难以获取时。

### 归纳迁移学习

训练数据集期望风险( 即真实数据分布上的错误率 )最小

归纳迁移学习的两种**迁移方式**:

- **模型微调（Fine-Tuning）**：
  
  - **预训练-微调**：在源领域上训练一个模型（预训练），然后将这个预训练模型应用到目标领域，通过在目标领域的数据上进一步训练（微调）来优化模型。
  - **冻结与微调**：冻结预训练模型的前几层，只在目标领域数据上训练模型的后几层或者输出层。

- **特征迁移**：
  
  - **特征重用**：直接使用在源领域上学到的特征表示作为目标领域的输入特征，然后在目标领域上训练一个新的分类器。
  - **特征变换**：通过某种映射方法将源领域和目标领域的特征变换到一个共同的特征空间，从而能够利用源领域的知识。
