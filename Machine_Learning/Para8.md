# 第八章: 注意力机制与外部记忆

人脑通过**注意力( attention )** 来解决信息超载的问题

## 自上而下vs自下而上的注意力

### 如何实现?

自下而上

* 汇聚( pooling )

自上而下

* 会聚( focus )

## 软性注意力机制

可以分为两步:

1. 计算注意力分布$\alpha $
   
   * $$
     \alpha_n=p(z=n|X,q)\\
=softmax(s(x_n,q))\\
=\frac{exp(s(x_n,q))}{\sum^N_{j=1}exp(s(x_j,q))}
     $$

2. 根据$\alpha $来计算输入信息的加权平均
   
   $$
   att(X,q)=\sum^N_{n=1}\alpha_n x_n\\
=\Bbb{E}_{z~p(z|X,q)}[X_z]
   $$

## 注意力打分函数:(考: 缩放点积)

![2024-06-10_08-59.png](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/Machine_Learning/2024-06-10_08-59.png)

顺带一提**softmax**函数:

与之对应的是**hardmax**函数, 即在向量$\overrightarrow{x}=(x_1,x_2,..,x_n) $中唯一选定一个最大值

但这与实际并不符合. 如在阅读关于一篇文章时, 他可能与多个主题都有关联, 因此我们想得到的是, 每个关键词对应的概率是多少.同时引入指数函数进行计算.

## (不考,重理解)说一下注意力机制的大体流程

具体来说，给定一个输入序列 $X=[x_1​,x_2​,…,x_n​]$，其中 xi​ 是序列中第 i 个元素的表示（通常是一个向量），自注意力机制的主要步骤包括以下几个部分：

1. **生成查询（Query）、键（Key）和值（Value）向量：** 每个输入向量 $x_i​$ 会通过线性变换分别生成查询向量 $q_i​$，键向量 $k_i$ 和值向量 $v_i$​。这些变换由学习得到的权重矩阵 $W_Q​，W_K​，W_V$​ 实现：
   
   $q_i​=W_Q​x_i​,k_i​=W_K​x_i​,v_i​=W_V​x_i​$

2. **计算注意力权重：** 对于每个查询向量 $q_i​$，通过与所有键向量 $k_j$​ 的点积计算相似度（即注意力得分），然后使用 softmax 函数将这些得分归一化以得到权重：
   
   $Attention(q_i​,K,V)=softmax(\frac{q_iK^T}{\sqrt{d_K}})$
   
   其中，K 是所有键向量组成的矩阵，V 是所有值向量组成的矩阵，$d_k$是键向量的维度，$d_k$​​ 是缩放因子，用于缓解点积值过大的问题。

3. **计算输出：** 上一步得到的权重向量与值向量 vj​ 进行加权求和，得到最终的输出向量：23
   
   $z_i=\sum_j \alpha_{ij}v_j $
   
   其中，$\alpha_{ij}$ 是第 i 个查询向量 $q_i$与第 j 个键向量 $k_j$​ 的注意力权重。

通过这种方式，自注意力机制能够根据序列中不同位置的相似度动态地调整每个位置的表示，捕捉序列中元素之间的长程依赖关系。这种机制在处理自然语言处理任务（如机器翻译、文本生成等）中尤为有效。

![2024-06-10_09-55.png](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/Machine_Learning/2024-06-10_09-55.png)

## 那为什么要生成查询(query),键(key)和值(value)三种向量呢?

- **查询向量（Query）：** 查询向量用于表示当前需要注意的输入元素信息。它表示了当前元素对其他元素的“关注”需求。换句话说，查询向量是在问“我（当前元素）应该关注哪些其他元素？”

- **键向量（Key）：** 键向量表示所有输入元素的信息，充当查询向量用来对比的对象。键向量用于衡量其他元素与查询向量的匹配程度，即回答“哪些元素与当前查询向量最相关？”

- **值向量（Value）：** 值向量表示要组合的信息。根据查询向量和键向量的匹配程度（注意力权重），对值向量进行加权求和，从而得到最终的输出。值向量回答“对于匹配的键向量，我需要组合哪些信息？”
  
  ![2024-06-10_10-18.png](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/Machine_Learning/2024-06-10_10-18.png)

### 举个具体的例子

假设我们在处理一个句子中的某个单词，查询向量可能表示当前单词希望关注其他单词的语法信息，而键向量则表示每个单词所包含的语法信息的特征，值向量则包含每个单词的实际内容信息。通过这种机制，当前单词可以根据其他单词的语法信息来加权组合其他单词的内容信息，从而生成对当前单词的上下文感知表示。

## 多头注意力

利用多个查询 $Q = [q_1 ,⋯,q_M ]$，来并行地从输入信息中选取多组信息．每个注意力关注输入信息的不同部分．

## 自注意力模型的计算过程

![2024-06-10_10-20.png](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/Machine_Learning/2024-06-10_10-20.png)

## 多头自注意力机制

![2024-06-10_10-20_1.png](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/Machine_Learning/2024-06-10_10-20_1.png)

## Transformer其他操作

仅仅自注意力机制还不够

其他操作:

* 位置编码

* 层归一化

* 直连边

* 逐位的FNN

## Hopfield网络

**联想记忆模型, 作为一种记忆的存储和检索模型**
