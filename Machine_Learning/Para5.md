# 卷积神经网络(CNN)

先说说全连接神经网络的主要缺点:

1. 权重矩阵的参数非常多

2. 局部不变性特征:
   
   * 自然图像中的物体都具有局部不变性的特征, 即尺度缩放, 平移, 旋转等操作不影响语义.
   
   * 全连接前馈网络很难提取这些局部不变性

## 卷积神经网络的概念



* 一种前馈神经网络

* 受生物学上**感受野( Receptive Field )**的机制提出
  
  * 在视觉神经系统中, 一个神经元的**感受野**是指视网膜上的特定区域, 只有这个区域内的刺激才能够激活神经元.

## 卷积神经网络有三个结构上的特性:

* 局部连接

* 权重共享

* 空间或时间上的次采样

## 滑动步长S和零填充P的作用

**用于调整输出大小,等宽卷积**

注:

我们做如下约定:

* i: 输入尺寸input

* o: 输出尺寸output

* s: 步长stride

* p: 填充padding( 一般是0 )

* k: 卷积核大小( kernel )大小
1. stride=1时
   
   * 没有padding:o=(i-k)+1
   
   * 有填充:o=(i-k)+2p+1

2. stride不等于1时,
   
   * $o=\lfloor \frac{i+2p-k}{s}\rfloor +1$

等宽卷积 : 步长S = 1 ，两端补零 𝑃 =(𝐾 − 1)/2 ，卷积后输出长度 𝑀 

## 卷积网络结构

**由卷积层, 汇聚层( 池化层 ), 全连接层交叉堆叠而成**

1. 卷积层:
   
   * 卷积层是CNN的核心组成部分，负责从输入层中提取特征。卷积层的每个神经元都与输入层的一个局部区域相连，通过学习局部特征来识别图像中的边缘、纹理等特征。卷积层的操作涉及到卷积核，也称为过滤器或权重，通过与输入数据进行卷积运算来提取特征。
     
     在卷积运算中，卷积核沿着输入数据滑动，并与输入数据对应位置的局部区域进行元素相乘并求和，得到该位置的特征值。通过这种方式，卷积层能够自动学习到图像中的特征表示。

2. 池化层
   
   * 池化层通常位于卷积层之后，用于减小数据的维度，减少计算量和过拟合的风险。池化操作可以是最大池化、平均池化或其他类型的池化方法。最大池化是指取局部区域中的最大值作为输出，平均池化则是取平均值作为输出。通过池化操作，可以有效降低数据的维度，同时保留重要的特征信息。

3. 激活函数层
   
   * 激活函数层负责引入非线性特性，使得CNN能够更好地学习和识别复杂的图像特征。常用的激活函数有ReLU（Rectified Linear Unit）、Sigmoid和Tanh等。这些函数会对输入信号施加非线性变换，使得网络能够更好地学习和理解图像中的复杂模式。

4. 全连接层
   
   * 全连接层是CNN的最后一个层次，负责将前面层次学习到的特征组合起来进行分类或回归任务。全连接层的每个神经元都与前一层的所有神经元相连，通过前向传播算法将特征组合成最终的输出结果。在分类任务中，全连接层的输出通常会经过Softmax函数进行归一化处理，将输出转换为概率分布形式，以方便后续的分类操作。



**卷积, 池化, 转置卷积: 低维特征映射到高维特征**

**空洞卷积 : 增加输出单元的感受野**

**空洞卷积的概念 :** 在原本卷积核中添加0元素以增大卷积核大小,但卷积核中参数数量没变.

## 一些典型的卷积神经网络及其特点

1. Alexnet
   
   * 第一个现代深度卷积网络模型, 首次使用了很多现代深度卷积网络的一些技术方法
     
     * 使用GPU进行并行训练，采用了ReLU作为非线性激活函数，使用Dropout防止过拟合，使用数据增强
   
   * 5个卷积层、3个汇聚层和3个全连接层

2. Inception网络(v1)
   
   * Inception网络是由有多个inception模块和少量的汇聚层堆叠而成。
     
     * 在Inception网络中，一个卷积层包含多个不同大小的卷积操作，称为Inception模块。
   
   * Inception模块同时使用1 × 1、3 × 3、5 × 5等不同大小的卷积核，并将得到的特征映射在深度上拼接（堆叠）起来作为输出特征映射。

![2024-06-06_16-28.png](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/The_theory_of_optimization/Lagrange_duality/2024-06-06_16-28.png)

在**Inception模块(v3)** 中

* 用多层小卷积核替换大卷积核, 以减少计算量与参数
  
  * 使用两层3x3的卷积来替换v1中的5x5的卷积
  
  * 使用连续的nx1和1xn来替换nxn的卷积

## 残差网络ResNet

**解决普通网络随着训练的层数逐步加深时,训练误差不再减小反而上升.**

![2024-06-06_16-41.png](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/The_theory_of_optimization/Lagrange_duality/2024-06-06_16-41.png)

首先残差网络是由各个不同的残差单元组成, 每个残差单元所学习的是如下函数: 

![2024-06-06_16-43.png](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/The_theory_of_optimization/Lagrange_duality/2024-06-06_16-43.png)

假设在一个深度网络中，我们期望一个非线性单元（可以为一层或多层的卷积层）f(x,θ)去逼近一个目标函数为h(x)。

**Resnet解决网络退化的原理** : 如果某层网络冗余,我们希望该层能够学习到一个恒等映射函数也就是$h(x)=x $ 但是学习这样的函数很困难但学习$f(x)$要更简单,所以通过跳跃连接可以实现上述函数.
