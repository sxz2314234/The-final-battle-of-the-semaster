# 第七章:网络优化与正则化

## 神经网络优化的难点

* **结构差异大**
  
  * 没有通用的优化算法
  
  * 超参数多

* **非凸优化问题**
  
  * 参数初始化
  
  * 逃离局部最优

* **梯度消失( 爆炸 )问题**

## 神经网络优化的改善方法

* 更有效的**优化算法**来提高优化方法的效率和稳定性
  
  * 动态学习率调整
  
  * 梯度估计修正

* 更好的**参数初始化**方法、数据预处理方法来提高优化效率

* 修改**网络结构**来得到更好的优化地形
  
  * 优化地形（ Optimization Landscape ）指在高维空间中损失函数的曲面形状
  
  * 好的优化地形通常比较平滑
  
  * 使用 ReLU 激活函数、残差连接、逐层归一化等

* 使用更好的**超参数优化方法**

## 优化算法: 小批量随机梯度下降

![2024-06-08_10-05.png](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/Machine_Learning/2024-06-08_10-05.png)

由上图可知,该算法的关键因素是

* 小批量样本数量

* 梯度

* 学习率

**批量大小与学习率的关系** : 线性缩放关系

优化算法的改进 : 

* 学习率调整( 固定学习率衰减, 周期性学习率调整, 自适应学习率RMSprop )

* 梯度修正估计( 动量法 )

* 综合方法( Adam近似为动量法+RMSprop )

### 数据规范化 : 最小最大值规范化,标准化

- **标准化**：将数据转换为均值为0，标准差为1的分布，常用于基于距离的算法。
- **归一化**：将数据缩放到[0, 1]或[-1, 1]区间，常用于神经网络等需要输入数据在特定范围内的算法。

**逐层规范化**

* 批量规范化
  
  * 1. **计算小批量的均值和方差**：
       对于给定的小批量输入 x={x1​,x2​,…,xm​}，计算均值 μ 和方差 σ2：
       
       $$
       \mu_B=\frac{1}{m}\sum^m_{i=1}x_i\\
\sigma^2_B=\frac{1}{m}\sum^m_{i=1}{(x_i-\mu_B)^2}
       $$
       
       
    
    2. **规范化**：
       使用均值和方差对输入进行规范化：
       
       $$
       \hat{x_i}=\frac{x_i-\mu_B}{\sqrt{(\sigma_B^2+\xi )}}
       $$
       
       
       
       其中，ϵ 是一个小常数，用于防止除以零。
    
    3. **缩放和平移**：
       引入两个可学习的参数 γ 和 β，对规范化后的值进行缩放和平移：
       
       $$
       y_i=\gamma x_i+\beta
       $$
       
       
       
       这里，γ 和 β 是通过反向传播学习到的参数。

* 层规范化
  
  * 与批量规范化的计算是相同的
  
  * 唯一的不同是,参与运算的$x_i$变成了神经网络中每一层的输出$h_i$

### 正则化: 提高神经网络的泛化能力

所有损害优化的方法都是**正则化**

正则化的方法:

* 增加优化约束
  
  * L1/L2约束
    
    * ![2024-06-08_11-11.png](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/Machine_Learning/2024-06-08_11-11.png)
      
      
  
  * 数据增强 : 图像数据增强
    
    * 图像数据的增强主要是通过算法对图像进行转变，引入噪声等方法来增加数据的多样性。
    
    * 图像数据的增强方法：
    
    * 旋转（Rotation）：将图像按顺时针或逆时针方向随机旋转一定角度；
      翻转（Flip）：将图像沿水平或垂直方法随机翻转一定角度；
      缩放（Zoom In/Out）：将图像放大或缩小一定比例；
      平移（Shift）：将图像沿水平或垂直方法平移一定步长；
      加噪声（Noise）：加入随机噪声。

* 干扰优化过程
  
  * 提前终止
    
    * ![2024-06-08_11-11_1.png](/home/sxz/The%20Final%20Battle%20of%20the%20Semester/Machine_Learning/2024-06-08_11-11_1.png)
  
  * 权重衰减
  
  * 随机梯度下降
